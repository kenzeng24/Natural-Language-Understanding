{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 3: The ðŸ¤— Transformers Framework\n",
    "**Due: March 29, 9:30 AM**\n",
    "\n",
    "In this homework assignment, you will replicate [the BitFit experiments (Zaken et al., 2020)](https://aclanthology.org/2022.acl-short.1/), whose paper we read and analyzed together in class. You will first use the [ðŸ¤— Transformers framework](https://huggingface.co/docs/transformers/index) to fine-tune a [BERT$_\\text{tiny}$ model](https://huggingface.co/prajjwal1/bert-tiny) ([Turc et al., 2019](https://arxiv.org/abs/1908.08962); [Bhargava et al., 2021](https://aclanthology.org/2021.insights-1.18/)) on the IMDb dataset. You will then fine-tune the same model, but with all parameters frozen other than the bias terms. You will compare the two models on the following metrics: (1) their accuracy on the IMDb test set and (2) the number of parameters trained during fine-tuning.\n",
    "\n",
    "## Important: Read Before Starting\n",
    "\n",
    "In the following exercises, you will need to implement functions defined in the `train_model.py` and `test_model.py` scripts, and you will need to write job scripts called `train_job.slurm` and `test_job.slurm` that will run your code on HPC. **Please write all your code in those files.** You should not submit this notebook with your solutions, and we will not grade it if you do. Please be aware that code written in a Jupyter notebook may run differently when copied into Python modules.\n",
    "\n",
    "The outputs shown in this notebook are the outputs that you should get **when all problems have been completed correctly**. You may obtain different results if you attempt to run the code cells before you have completed the problem set, or if you have completed one or more problems incorrectly.\n",
    "\n",
    "For part of this assignment, you will be asked to run code on HPC. You will not be able to run code for those problems in Jupyter Notebook.\n",
    "\n",
    "To begin, please run the following `import` statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9e9b3523e220>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Model and tokenizer from ðŸ¤— Transformers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mBertForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertTokenizerFast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from collections.abc import Iterable\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Model and tokenizer from ðŸ¤— Transformers\n",
    "from transformers import AutoModelForSequenceClassification, \\\n",
    "    BertForSequenceClassification, BertTokenizerFast\n",
    "\n",
    "# Code you will write for this assignment\n",
    "from train_model import init_model, preprocess_dataset, init_trainer\n",
    "from test_model import init_tester"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Setup (70 Points in Total)\n",
    "\n",
    "In this assignment, you will fine-tune a pre-trained Transformer model using libraries provided by [Hugging Face](https://huggingface.co/) (whose name is usually styled using the emoji ðŸ¤—). You have already been exposed to Hugging Face in HW 2, where you used the [ðŸ¤— Datasets](https://huggingface.co/docs/datasets/index) library to load the IMDb dataset. In the following problems, you will use two additional Hugging Face libraries:\n",
    "- [ðŸ¤— Transformers](https://huggingface.co/docs/transformers/index), which provides pre-trained Transformers implemented as PyTorch modules, as well as code for training and testing models, and \n",
    "- [ðŸ¤— Evaluate](https://huggingface.co/docs/evaluate/index), which provides evaluation metrics such as accuracy and F1.\n",
    "\n",
    "For several parts of this problem, you will need to refer to the [Hugging Face fine-tuning tutorial](https://huggingface.co/docs/transformers/training) for guidance.\n",
    "\n",
    "### Problem 1a: Understand the ðŸ¤— Transformers Library (No Submission, 0 Points)\n",
    "\n",
    "ðŸ¤— Transformers is imported into Python via the name `transformers`. Please find the import statements from ðŸ¤— Transformers in the code cell above.\n",
    "\n",
    "ðŸ¤— Transformers comes with a number of different Transformer architectures, as well as [the Model Hub, a repository of pre-trained model parameters](https://huggingface.co/models). A pre-trained model is loaded by calling the model architecture's `.from_pretrained` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5969e2b8b5d244ce8fdddd142c042dd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5165047bcb6e456c930521d163be15d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)\"pytorch_model.bin\";:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                      num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above loads a Transformer classifier consisting of a pre-trained BERT$_\\text{base}$ encoder with case-insensitive vocabulary and a randomly initialized 2-layer MLP decoder with tanh activation. The choice of this particular set of pre-trained parameters is specified by the identifier `'bert-base-uncased'`, which is passed to the first parameter of `.from_pretrained`. Different pre-trained weights can be loaded by passing a different identifier to `.from_pretrained`. The following code loads the BERT$_\\text{tiny}$ model from [Turc et al. (2019)](https://arxiv.org/abs/1908.08962) and [Bhargava et al. (2021)](https://aclanthology.org/2021.insights-1.18/), which you will be fine-tuning in this assignment. (The `/` indicates that this is a user-submitted model, uploaded by the user [`prajjwal1`](https://huggingface.co/prajjwal1).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1304db51c28c4d448b4947ae8ba7a45b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/285 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a94ea7228154c118cde18906a28624e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)\"pytorch_model.bin\";:   0%|          | 0.00/17.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"prajjwal1/bert-tiny\", num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to load a model using the code above, you would have to know that BERT$_{\\text{tiny}}$'s architecture is implemented using the same class as BERT$_{\\text{base}}$. This is not true in general, however. For instance, if you wanted to initialize a RoBERTa classifier instead of a BERT classifier, you would need to call `RobertaForSequenceClassification.from_pretrained` instead of `BertForSequenceClassification.from_pretrained`. When you don't know which class implements the architecture of pre-trained model you want to load, you can use the `AutoModelForSequenceClassification` class ([and equivalent classes for other tasks](https://huggingface.co/docs/transformers/model_doc/auto)), which will figure out which class to instantiate based on the pre-trained weights you would like to load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# This code does exactly the same thing as the previous code cell\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"prajjwal1/bert-tiny\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-065ed5fcd4ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.named_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to models, ðŸ¤— Transformers also provides tokenizers that implement a full processing pipeline similar to what you implemented in HW 2. You can load the appropriate tokenizer for your model using a `.from_pretrained` method, just as you did with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7393dca83b6c4ae98fde1e1e6f73acfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"prajjwal1/bert-tiny\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like in HW 2, the tokenizer object can be called as a function. Doing so will return a fully processed input, ready to be passed to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 7592, 2088,  999,  102,    0],\n",
       "        [ 101, 2129, 2024, 2017, 1029,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Because ðŸ¤— Transformers supports multiple deep learning libraries, you will\n",
    "# need to use the keyword parameter return_tensors in order to indicate that\n",
    "# you want your inputs to be returned in PyTorch format.\n",
    "inputs = tokenizer([\"Hello world!\", \"How are you?\"], padding=True, \n",
    "                   return_tensors=\"pt\")\n",
    "inputs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inputs returned by the tokenizer are passed to the model via [dictionary unpacking](https://realpython.com/python-kwargs-and-args/). The output of the model is structured, with various kinds of information provided depending on keyword arguments passed to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.2078,  0.0662],\n",
      "        [-0.0285, -0.0969]]), hidden_states=None, attentions=None)\n",
      "\n",
      "tensor([[ 0.2078,  0.0662],\n",
      "        [-0.0285, -0.0969]])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "print(outputs, end=\"\\n\\n\")\n",
    "\n",
    "# Use the dot operator to access parts of the output\n",
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1b: Understand BERT Inputs (Written, 10 Points)\n",
    "\n",
    "Look at the tokenized inputs from two code cells above. The inputs are represented as a dict with three keys: `'input_ids'`, `'token_type_ids'`, and `'attention_mask'`. What do each of those three inputs represent? Please consult the [original BERT paper (Devlin et al., 2018)](https://arxiv.org/abs/1810.04805) for guidance.\n",
    "\n",
    "### Problem 1c: Understand BERT Hyperparameters (Written, 10 Points)\n",
    "\n",
    "For this assignment, you will perform hyperparameter tuning for the BERT$_\\text{tiny}$ model using the same procedure as in the [original paper](https://arxiv.org/abs/1908.08962). Their hyperparameter tuning procedure is documented in the [official BERT GitHub repository](https://github.com/google-research/bert) under the heading \"**\\*\\*\\*\\*\\*New March 11th, 2020: Smaller BERT Models\\*\\*\\*\\*\\***.\" Please read this documentation and describe how hyperparameter tuning was performed for the GLUE benchmark.\n",
    "\n",
    "### Problem 1d: Prepare Dataset (Code, 10 Points)\n",
    "\n",
    "As in HW 2, we will again be using the IMDb dataset provided by ðŸ¤— Datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53958f1d6d4e4abcbac298f1c318c8ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.31k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f2388405d7644ea8c5022a0a2a6d742",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/2.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8e03f7a527d4f05bef2933f74f49590",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.59k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset imdb/plain_text to PATH REDACTED...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17583f36f24e42ae81d422ec19ba7235",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/84.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset imdb downloaded and prepared to PATH REDACTED. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad5276d4d2c14e048d07a8d350a5adf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load IMDb dataset and create validation split\n",
    "imdb = load_dataset(\"imdb\")\n",
    "split = imdb[\"train\"].train_test_split(.2, seed=3463)\n",
    "imdb[\"train\"] = split[\"train\"]\n",
    "imdb[\"val\"] = split[\"test\"]\n",
    "del imdb[\"unsupervised\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ðŸ¤— Transformers fine-tuning API expects datasets to be pre-processed through the following steps.\n",
    "- All input texts should be tokenized.\n",
    "- BERT models have a maximum input length, and all inputs need to be truncated to this length.\n",
    "- Inputs shorter than the maximum input length should be padded to this length.\n",
    "- The pre-processed inputs do not need to be in the form of PyTorch tensors.\n",
    "\n",
    "These steps are performed by the `preprocess_dataset` function in `run_experiment.py`, which you will implement for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e326ae5497c2472782f9019e522d26db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03ec9167a0b742ec8ead53489d585798",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e12501a81f84095b92e798bc6065b77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:\n",
      "<class 'list'>\n",
      "['As so many others ha', 'When converting a bo']\n",
      "\n",
      "label:\n",
      "<class 'list'>\n",
      "[1, 0]\n",
      "\n",
      "input_ids:\n",
      "<class 'list'>\n",
      "[[101, 2004, 2061, 2116, 2500, 2031, 2517, 1010, 2023, 2003, 1037, 6919, 4516, 1012, 2182, 2003, 1037, 2862, 1997, 1996], [101, 2043, 16401, 1037, 2338, 2000, 2143, 1010, 2009, 2003, 3227, 1037, 2204, 2801, 2000, 2562, 2012, 2560, 2070, 1997]]\n",
      "\n",
      "token_type_ids:\n",
      "<class 'list'>\n",
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "\n",
      "attention_mask:\n",
      "<class 'list'>\n",
      "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imdb[\"train\"] = preprocess_dataset(imdb[\"train\"], tokenizer)\n",
    "imdb[\"val\"] = preprocess_dataset(imdb[\"val\"], tokenizer)\n",
    "imdb[\"test\"] = preprocess_dataset(imdb[\"test\"], tokenizer)\n",
    "\n",
    "# Visualize the preprocessed dataset\n",
    "for k, v in imdb[\"val\"][:2].items(): \n",
    "    print(\"{}:\\n{}\\n{}\\n\".format(k, type(v),\n",
    "                                 [item[:20] if isinstance(item, Iterable) else \n",
    "                                 item for item in v[:5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please base your implementation on the [Hugging Face fine-tuning tutorial](https://huggingface.co/docs/transformers/training), and please consult [Appendix A.2 of the BERT paper](https://arxiv.org/abs/1810.04805) to find out what the maximum input length should be.\n",
    "\n",
    "### Problem 1e: Freeze Non-Bias Weights (Code, 10 Points)\n",
    "\n",
    "At the end of this assignment, you will be comparing a BERT$_{\\text{tiny}}$ model fine-tuned using BitFit to a BERT$_{\\text{tiny}}$ model fine-tuned _without_ BitFit. To run that experiment, you will need to support freezing all non-bias parameters of the model. To do this, please implement the `init_model` function, illustrated below. This function should load a pre-trained BERT classifier model from the Hugging Face Model Hub and optionally freeze non-bias parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# The first parameter is unused; we just pass None to it\n",
    "model = init_model(None, \"prajjwal1/bert-tiny\", use_bitfit=True)\n",
    "\n",
    "# Check if weight matrix is frozen\n",
    "print(model.bert.encoder.layer[0].attention.self.query.weight.requires_grad)\n",
    "\n",
    "# Check if bias term is frozen\n",
    "print(model.bert.encoder.layer[0].attention.self.query.bias.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint:** Please consult the [documentation for the function `nn.Module.named_parameters`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.named_parameters).\n",
    "\n",
    "### Problem 1f: Set Up Trainer and Tester (Code, 15 Points)\n",
    "\n",
    "ðŸ¤— Transformers provides a [`Trainer` object](https://huggingface.co/docs/transformers/main_classes/trainer) that implements training and testing a neural network. For this problem, please implement the functions `init_trainer` in `train_model.py` and `init_tester` in `test_model.py`, which will set up the `Trainer`s used to train and test your model, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'init_trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-acea84a50041>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Creates a Trainer from a Hugging Face Model Hub identifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_trainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"prajjwal1/bert-tiny\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimdb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimdb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Creates a Trainer to test a Hugging Face saved model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# tester = init_tester(\"checkpoints/run-13/checkpoint-1252\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'init_trainer' is not defined"
     ]
    }
   ],
   "source": [
    "# Creates a Trainer from a Hugging Face Model Hub identifier\n",
    "trainer = init_trainer(\"prajjwal1/bert-tiny\", imdb[\"train\"], imdb[\"val\"])\n",
    "\n",
    "# Creates a Trainer to test a Hugging Face saved model\n",
    "# tester = init_tester(\"checkpoints/run-13/checkpoint-1252\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your `init_trainer` function needs to support the following.\n",
    "- The training configuration (total number of epochs, early stopping criteria if any) must match your answer for Problem 1c.\n",
    "- Your `Trainer` needs to save the model obtained during each training run to a folder called `checkpoints`.\n",
    "- You should leave the `model` keyword parameter blank and instead pass an argument to the `model_init` keyword parameter.\n",
    "- It should evaluate models based on accuracy.\n",
    "\n",
    "Your `init_tester` function needs to support the following.\n",
    "- The `Trainer` should only support testing and not traiing.\n",
    "- It should evaluate models based on accuracy.\n",
    "\n",
    "\n",
    "Please use the [Hugging Face fine-tuning tutorial](https://huggingface.co/docs/transformers/training) as well as [this forum post](https://discuss.huggingface.co/t/using-trainer-at-inference-time/9378/3) for guidance. You may need to create new functions for this problem, and you may find it useful to learn about [lambda expressions](https://realpython.com/python-lambda/) if you don't know about them already.\n",
    "\n",
    "### Problem 1g: Set Up Hyperparameter Tuning (Code, 15 Points)\n",
    "\n",
    "Finally, to complete the experiment setup, you will implement hyperparameter tuning using the [Optuna](https://optuna.org/) framework. Optuna is integrated with ðŸ¤— Transformers, and it can be invoked via the `Trainer.hyperparameter_search` method. Please implement the function `hyperparameter_search_settings` in `train_model.py` by returing the correct keyword arguments for `Trainer.hyperparameter_search`. (Observe that, at the end of `train_model.py`, these keyword arguments are passed to `Trainer.hyperparameter_search` via dictionary unpacking.)  \n",
    "\n",
    "Your code should support the following requirements.\n",
    "- Your hyperparameter tuning configuration must match your answer for Problem 1c.\n",
    "- You must use Optuna for hyperparameter tuning.\n",
    "- You must indicate to Optuna that the hyperparameter search should maximize accuracy.\n",
    "\n",
    "Please use the following resources for guidance.\n",
    "- [The Hugging Face tutorial on hyperparameter tuning](https://huggingface.co/docs/transformers/hpo_train)\n",
    "- [The documentation for `Trainer.hyperparameter_search`](https://huggingface.co/docs/transformers/v4.26.1/en/main_classes/trainer#transformers.Trainer.hyperparameter_search)\n",
    "- [The documentation for Optuna's `GridSampler`](https://optuna.readthedocs.io/en/v2.0.0/reference/generated/optuna.samplers.GridSampler.html)\n",
    "\n",
    "## Problem 2: Run Experiment (30 Points in Total)\n",
    "\n",
    "To complete the assignment, you will now run your code on HPC using Google Cloud bursting, which we learned how to do in the Week 7 lab.\n",
    "\n",
    "### Problem 2a: Prepare Slurm Batch Job (Code, 10 Points)\n",
    "\n",
    "Please prepare a Slurm batch job for fine-tuning a BERT$_{\\text{tiny}}$ model by filling in the template provided in `train_job.slurm`. Your batch job should activate a Conda environment with the appropriate libraries installed and run the `train_model.py` script using one GPU. It should take roughly 20 to 60 minutes to fine-tune a model with all hyperparameter tuning trials.\n",
    "\n",
    "If you would like, you may choose to customize the Slurm options \n",
    "```\n",
    "#SBATCH --job-name=train_model\n",
    "#SBATCH --output=/scratch/${USER}/%j_%x.out\n",
    "#SBATCH --error=/scratch/${USER}/%j_%x.err\n",
    "```\n",
    "which control the name of your job and the locations of the job's standard output and error logs, respectively. You may also add the options\n",
    "```\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH --mail-user=<your email address>\n",
    "```\n",
    "which will cause Slurm to send you an email when your job starts and ends.\n",
    "\n",
    "For this problem, please submit the batch job used to fine-tune a BERT$_{\\text{tiny}}$ model in the next problem, either with or without BitFit. You may also optionally write a batch job to test a model by running `test_model.py`, but since testing is fast enough to run on your personal computer, you are not required to submit it.\n",
    "\n",
    "### Problem 2b: Train Models (Code and Written, 10 Points)\n",
    "\n",
    "Please run the following two jobs on HPC:\n",
    "- a job to fine-tune a BERT$_{\\text{tiny}}$ model on the IMDb dataset _with_ BitFit\n",
    "- a job to fine-tune a BERT$_{\\text{tiny}}$ model on the IMDb dataset _without_ BitFit.\n",
    "\n",
    "Please submit the standard output logs for both jobs using the filenames `train_with_bitfit.out` and `train_without_bitfit.out`, respectively.\n",
    "\n",
    "The `train_model.py` script should create a Pickle object containing information about the best hyperparameters found during hyperparameter tuning. Please submit this object, using the filenames `train_results_with_bitfit.p` and `train_results_without_bitfit.p` for your two training runs, respectively. Please also report the highest validation accuracy attained in each of your two training runs, as well as the hyperparameters used in those trials. Please format these results as a table such as the following.\n",
    "\n",
    "| | Validation Accuracy | Learning Rate | Batch Size |\n",
    "|---|---|---|---|\n",
    "| Without BitFit | | | |\n",
    "| With BitFit | | | |\n",
    "\n",
    "**Important:** Please make sure to `ssh burst` before submitting your batch job!\n",
    "\n",
    "### Problem 2c: Test Models and Report Results (Code and Written, 10 Points)\n",
    "\n",
    "For each of your two training runs, please test the model that attained the best validation accuracy across all hyperparameter tuning trials. You may do so by running the `test_model.py` script on HPC or on your personal computer. Once testing is complete, please report your results in the form of a table such as the following.\n",
    "\n",
    "| | # Trainable Parameters | Test Accuracy | \n",
    "|---|---|---|\n",
    "| Without BitFit | | | \n",
    "| With BitFit | | | \n",
    "\n",
    "The `test_model.py` script should create a Pickle object containing information about test results. Please submit this object, using the filenames `test_results_with_bitfit.p` and `test_results_without_bitfit.p` for your two tests.\n",
    "\n",
    "Finally, please comment on your results. How do they compare to the results reported by Zaken et al. (2020)? What does this say about BitFit and its applicability to other pre-trained Transformers?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
